{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Science Blog Post: Predicting GDP per Capita\n",
        "\n",
        "**Following the CRISP-DM Process**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Business Understanding\n",
        "\n",
        "### Brief Description\n",
        "\n",
        "This project analyzes the relationship between various macroeconomic indicators and GDP per capita across multiple countries from 2010-2023. Using World Bank data, we aim to understand which factors most significantly influence economic prosperity and build predictive models to estimate GDP per capita based on these indicators.\n",
        "\n",
        "The analysis covers **40+ macroeconomic indicators** including:\n",
        "- \ud83d\udcda Education & Human Capital\n",
        "- \ud83d\udcb0 Trade & Economic Structure\n",
        "- \ud83c\udfe5 Healthcare Investment\n",
        "- \ud83c\udf10 Technology & Infrastructure\n",
        "- \ud83c\udfdb\ufe0f Governance Quality\n",
        "- \ud83d\udc65 Demographics & Urbanization\n",
        "- \ud83d\udcb3 Financial Development\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Research Questions\n",
        "\n",
        "**Question 1:** What are the strongest predictors of GDP per capita among technology, education, and healthcare indicators?\n",
        "\n",
        "**Question 2:** How do trade openness and economic structure (exports, imports, FDI) relate to economic prosperity?\n",
        "\n",
        "**Question 3:** Which countries outperform or underperform relative to model predictions, and what can we learn from these outliers?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. Data Understanding\n",
        "\n",
        "### Access and Explore\n",
        "\n",
        "We'll retrieve comprehensive data from the World Bank API covering 20 major economies across 14 years (2010-2023).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "# World Bank API\n",
        "import wbdata\n",
        "from datetime import datetime\n",
        "\n",
        "# Configure display\n",
        "pd.set_option('display.float_format', lambda x: f'{x:,.2f}')\n",
        "pd.set_option('display.max_columns', None)\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "\n",
        "print(\"\u2713 Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define comprehensive indicator set\n",
        "indicators = {\n",
        "    # TARGET\n",
        "    'NY.GDP.PCAP.PP.KD': 'GDP per capita, PPP (constant intl $)',\n",
        "    \n",
        "    # BASIC INDICATORS\n",
        "    'SP.DYN.LE00.IN': 'Life expectancy at birth',\n",
        "    'SL.UEM.TOTL.ZS': 'Unemployment rate (%)',\n",
        "    'SL.TLF.TOTL.IN': 'Labor force, total',\n",
        "    'FP.CPI.TOTL.ZG': 'Inflation rate (CPI %)',\n",
        "    'SP.POP.TOTL': 'Population',\n",
        "    'SP.POP.GROW': 'Population growth (%)',\n",
        "    \n",
        "    # EDUCATION & HUMAN CAPITAL\n",
        "    'SE.XPD.TOTL.GD.ZS': 'Education expenditure (% of GDP)',\n",
        "    'SE.SEC.ENRR': 'Secondary school enrollment rate',\n",
        "    'SE.TER.ENRR': 'Tertiary school enrollment rate',\n",
        "    'SE.ADT.LITR.ZS': 'Adult literacy rate',\n",
        "    \n",
        "    # TRADE & ECONOMIC STRUCTURE\n",
        "    'NE.TRD.GNFS.ZS': 'Trade (% of GDP)',\n",
        "    'NE.EXP.GNFS.ZS': 'Exports (% of GDP)',\n",
        "    'NE.IMP.GNFS.ZS': 'Imports (% of GDP)',\n",
        "    'BX.KLT.DINV.WD.GD.ZS': 'FDI net inflows (% of GDP)',\n",
        "    'NV.IND.TOTL.ZS': 'Industry value added (% of GDP)',\n",
        "    'NV.AGR.TOTL.ZS': 'Agriculture value added (% of GDP)',\n",
        "    'NV.SRV.TOTL.ZS': 'Services value added (% of GDP)',\n",
        "    \n",
        "    # HEALTH\n",
        "    'SH.XPD.CHEX.GD.ZS': 'Health expenditure (% of GDP)',\n",
        "    'SP.DYN.IMRT.IN': 'Infant mortality rate',\n",
        "    'SH.DYN.MORT': 'Under-5 mortality rate',\n",
        "    \n",
        "    # INFRASTRUCTURE & TECHNOLOGY\n",
        "    'IT.NET.USER.ZS': 'Internet users (% of population)',\n",
        "    'IT.CEL.SETS.P2': 'Mobile subscriptions per 100 people',\n",
        "    'EG.USE.ELEC.KH.PC': 'Electric power consumption (kWh per capita)',\n",
        "    \n",
        "    # DEMOGRAPHICS & URBANIZATION\n",
        "    'SP.URB.TOTL.IN.ZS': 'Urban population (% of total)',\n",
        "    'SP.POP.DPND': 'Age dependency ratio',\n",
        "    'SP.POP.65UP.TO.ZS': 'Population ages 65+ (% of total)',\n",
        "    'SP.POP.0014.TO.ZS': 'Population ages 0-14 (% of total)',\n",
        "    'SP.DYN.TFRT.IN': 'Fertility rate (births per woman)',\n",
        "    \n",
        "    # FINANCIAL DEVELOPMENT\n",
        "    'FD.AST.PRVT.GD.ZS': 'Domestic credit to private sector (% of GDP)',\n",
        "    'FS.AST.DOMS.GD.ZS': 'Domestic credit (% of GDP)',\n",
        "    \n",
        "    # ENERGY & ENVIRONMENT\n",
        "    'EG.USE.PCAP.KG.OE': 'Energy use per capita',\n",
        "\n",
        "}\n",
        "\n",
        "print(f\"\ud83d\udcca Total indicators to retrieve: {len(indicators)}\")\n",
        "print(\"\\nIndicator categories:\")\n",
        "print(\"  \u2022 Basic macroeconomic: 7\")\n",
        "print(\"  \u2022 Education & human capital: 4\")\n",
        "print(\"  \u2022 Trade & economic structure: 7\")\n",
        "print(\"  \u2022 Health: 3\")\n",
        "print(\"  \u2022 Infrastructure & technology: 3\")\n",
        "print(\"  \u2022 Demographics & urbanization: 5\")\n",
        "print(\"  \u2022 Financial development: 2\")\n",
        "print(\"  \u2022 Energy & environment: 2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Countries to analyze (expanded list)\n",
        "COUNTRIES = [\n",
        "    \"USA\", \"CHN\", \"JPN\", \"DEU\", \"GBR\", \"FRA\", \"IND\", \"ITA\", \"BRA\", \"CAN\",\n",
        "    \"KOR\", \"ESP\", \"MEX\", \"IDN\", \"NLD\", \"SAU\", \"TUR\", \"CHE\", \"POL\", \"ARG\"\n",
        "]\n",
        "\n",
        "# Date range\n",
        "start_year, end_year = \"2010\", \"2023\"\n",
        "\n",
        "print(f\"\ud83c\udf0d Fetching data for {len(COUNTRIES)} countries from {start_year} to {end_year}...\")\n",
        "print(\"\u23f3 This may take 2-3 minutes due to API rate limits...\\n\")\n",
        "\n",
        "try:\n",
        "    raw_df = wbdata.get_dataframe(\n",
        "        indicators, \n",
        "        country=COUNTRIES, \n",
        "        parse_dates=True, \n",
        "        date=(start_year, end_year)\n",
        "    )\n",
        "    print(\"\u2713 Data successfully retrieved from World Bank API!\")\n",
        "except Exception as e:\n",
        "    print(f\"\u274c Error fetching data: {e}\")\n",
        "    print(\"Tip: Check internet connection or try reducing the number of countries\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reset index and process dates\n",
        "df = raw_df.reset_index()\n",
        "\n",
        "if not np.issubdtype(df['date'].dtype, np.datetime64):\n",
        "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "df['year'] = df['date'].dt.year\n",
        "\n",
        "# Define target\n",
        "target_col = 'GDP per capita, PPP (constant intl $)'\n",
        "\n",
        "# Initial data quality check\n",
        "print(f\"\\n\ud83d\udccb Initial dataset shape: {df.shape}\")\n",
        "print(f\"\ud83d\udcc5 Date range: {df['year'].min()} - {df['year'].max()}\")\n",
        "print(f\"\ud83c\udf0d Countries: {df['country'].nunique()}\")\n",
        "print(f\"\\n\ud83d\udcca Missing data before cleaning:\")\n",
        "missing_summary = df.isna().sum().sort_values(ascending=False)\n",
        "print(missing_summary[missing_summary > 0].head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initial Data Exploration\n",
        "\n",
        "Let's examine the structure, quality, and basic statistics of our dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics\n",
        "print(\"\ud83d\udcca Summary Statistics:\\n\")\n",
        "display(df[[target_col] + base_feature_cols[:10]].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation analysis with target\n",
        "numeric_cols = df[[target_col] + base_feature_cols].select_dtypes(include=[np.number]).columns\n",
        "correlations = df[numeric_cols].corr()[target_col].sort_values(ascending=False)\n",
        "\n",
        "print(\"\ud83c\udfaf Top 15 Positive Correlations with GDP per Capita:\\n\")\n",
        "display(correlations.head(16)[1:])  # Exclude self-correlation\n",
        "\n",
        "print(\"\\n\ud83c\udfaf Top 10 Negative Correlations with GDP per Capita:\\n\")\n",
        "display(correlations.tail(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize top correlations\n",
        "top_features = correlations.abs().sort_values(ascending=False)[1:21]  # Top 20 excluding target\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "colors = ['green' if correlations[feat] > 0 else 'red' for feat in top_features.index]\n",
        "plt.barh(range(len(top_features)), correlations[top_features.index], color=colors, alpha=0.7)\n",
        "plt.yticks(range(len(top_features)), top_features.index)\n",
        "plt.xlabel('Correlation with GDP per Capita')\n",
        "plt.title('Top 20 Feature Correlations with GDP per Capita', fontsize=14, fontweight='bold')\n",
        "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. Prepare Data\n",
        "\n",
        "### Clean Data\n",
        "\n",
        "We'll handle missing values, remove outliers, and perform feature engineering to create meaningful predictors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed Missing Value Analysis\n",
        "print(\"\ud83d\udd0d COMPREHENSIVE MISSING VALUE ANALYSIS\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Calculate missing percentages\n",
        "missing_stats = pd.DataFrame({\n",
        "    'Column': df.columns,\n",
        "    'Missing_Count': df.isna().sum(),\n",
        "    'Missing_Percent': (df.isna().sum() / len(df)) * 100,\n",
        "    'Data_Type': df.dtypes\n",
        "})\n",
        "\n",
        "missing_stats = missing_stats[missing_stats['Missing_Count'] > 0].sort_values('Missing_Percent', ascending=False)\n",
        "\n",
        "print(f\"\\n\ud83d\udcca Dataset Overview:\")\n",
        "print(f\"   Total rows: {len(df):,}\")\n",
        "print(f\"   Total columns: {len(df.columns)}\")\n",
        "print(f\"   Columns with missing values: {len(missing_stats)}\")\n",
        "\n",
        "print(f\"\\n\ud83d\udcc9 Missing Value Summary:\\n\")\n",
        "if len(missing_stats) > 0:\n",
        "    display(missing_stats)\n",
        "    \n",
        "    # Categorize by severity\n",
        "    high_missing = missing_stats[missing_stats['Missing_Percent'] > 50]\n",
        "    medium_missing = missing_stats[(missing_stats['Missing_Percent'] > 20) & (missing_stats['Missing_Percent'] <= 50)]\n",
        "    low_missing = missing_stats[missing_stats['Missing_Percent'] <= 20]\n",
        "    \n",
        "    print(f\"\\n\u26a0\ufe0f Missing Value Severity Categories:\")\n",
        "    print(f\"   \ud83d\udd34 High (>50%): {len(high_missing)} columns\")\n",
        "    print(f\"   \ud83d\udfe1 Medium (20-50%): {len(medium_missing)} columns\")\n",
        "    print(f\"   \ud83d\udfe2 Low (<20%): {len(low_missing)} columns\")\n",
        "    \n",
        "    if len(high_missing) > 0:\n",
        "        print(f\"\\n   High missing columns: {', '.join(high_missing['Column'].tolist())}\")\n",
        "else:\n",
        "    print(\"   \u2713 No missing values detected!\")\n",
        "\n",
        "# Visualize missing data pattern\n",
        "if len(missing_stats) > 0:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    \n",
        "    # Bar plot of missing percentages\n",
        "    top_missing = missing_stats.head(15)\n",
        "    colors = ['red' if x > 50 else 'orange' if x > 20 else 'yellow' for x in top_missing['Missing_Percent']]\n",
        "    axes[0].barh(range(len(top_missing)), top_missing['Missing_Percent'], color=colors, alpha=0.7)\n",
        "    axes[0].set_yticks(range(len(top_missing)))\n",
        "    axes[0].set_yticklabels(top_missing['Column'])\n",
        "    axes[0].set_xlabel('Missing Percentage (%)', fontsize=11)\n",
        "    axes[0].set_title('Top 15 Columns by Missing Data Percentage', fontsize=12, fontweight='bold')\n",
        "    axes[0].axvline(x=20, color='orange', linestyle='--', linewidth=1, alpha=0.5, label='20% threshold')\n",
        "    axes[0].axvline(x=50, color='red', linestyle='--', linewidth=1, alpha=0.5, label='50% threshold')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(axis='x', alpha=0.3)\n",
        "    axes[0].invert_yaxis()\n",
        "    \n",
        "    # Missing data heatmap (sample of columns)\n",
        "    sample_cols = missing_stats.head(10)['Column'].tolist()\n",
        "    if target_col not in sample_cols:\n",
        "        sample_cols = [target_col] + sample_cols[:9]\n",
        "    \n",
        "    missing_matrix = df[sample_cols].isna().astype(int)\n",
        "    sns.heatmap(missing_matrix.T, cbar=False, cmap='RdYlGn_r', ax=axes[1])\n",
        "    axes[1].set_xlabel('Row Index (sample)', fontsize=11)\n",
        "    axes[1].set_ylabel('Features', fontsize=11)\n",
        "    axes[1].set_title('Missing Data Pattern (Top 10 Features)', fontsize=12, fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Check for missing data patterns by country\n",
        "print(\"\\n\\n\ud83c\udf0d Missing Data by Country:\\n\")\n",
        "country_missing = df.groupby('country').apply(lambda x: x.isna().sum().sum())\n",
        "country_missing_sorted = country_missing.sort_values(ascending=False).head(10)\n",
        "print(\"Top 10 countries with most missing values:\")\n",
        "for country, count in country_missing_sorted.items():\n",
        "    print(f\"   {country}: {count} missing values\")\n",
        "\n",
        "# Check if missingness is related to year\n",
        "if 'year' in df.columns:\n",
        "    print(\"\\n\\n\ud83d\udcc5 Missing Data by Year:\\n\")\n",
        "    year_missing = df.groupby('year').apply(lambda x: x.isna().sum().sum())\n",
        "    print(year_missing.to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Missing Value Handling Strategy & Justification\n",
        "\n",
        "Based on the analysis above, here is our well-reasoned approach to handling missing values:\n",
        "\n",
        "#### **1. Target Variable (GDP per Capita)**\n",
        "**Strategy:** Complete case deletion (remove rows with missing target)\n",
        "\n",
        "**Justification:**\n",
        "- We cannot impute the target variable as it would introduce bias into our model\n",
        "- These observations provide no value for supervised learning\n",
        "- The proportion of missing target values is typically low in World Bank data\n",
        "- This is standard practice in predictive modeling\n",
        "\n",
        "#### **2. Feature Variables - Multi-Tiered Approach**\n",
        "\n",
        "**Strategy A: Within-Country Median Imputation (Primary Method)**\n",
        "\n",
        "**Justification:**\n",
        "- **Preserves country-specific patterns**: Economic indicators vary significantly by country (e.g., USA vs India)\n",
        "- **Temporal consistency**: Using a country's own median maintains realistic values for that nation's economic context\n",
        "- **Robust to outliers**: Median is less affected by extreme values than mean\n",
        "- **Assumption**: Missing data within a country is Missing At Random (MAR) - the missingness doesn't depend on the unobserved value itself\n",
        "\n",
        "**Strategy B: Global Median Fallback**\n",
        "\n",
        "**Justification:**\n",
        "- Used only when country-specific median is unavailable (entire feature missing for a country)\n",
        "- Provides a reasonable central tendency estimate\n",
        "- Better than dropping entire features or countries\n",
        "- Acknowledges uncertainty by using a conservative estimate\n",
        "\n",
        "#### **3. Features with High Missingness (>50%)**\n",
        "\n",
        "**Strategy:** Retained with imputation, flagged for feature selection\n",
        "\n",
        "**Justification:**\n",
        "- World Bank data often has systematic missingness for certain indicators (e.g., newer metrics, country-specific reporting)\n",
        "- Feature selection algorithms will naturally downweight unreliable features\n",
        "- Some high-value features may have missingness but still provide signal when available\n",
        "- We document missingness patterns for transparency\n",
        "\n",
        "#### **Alternative Approaches Considered (and Why Not Used)**\n",
        "\n",
        "\u274c **Mean Imputation**: Sensitive to outliers and doesn't preserve country-specific distributions\n",
        "\n",
        "\u274c **Forward/Backward Fill**: Inappropriate for cross-sectional analysis; would assume values constant over time\n",
        "\n",
        "\u274c **Predictive Imputation (MICE, KNN)**: \n",
        "   - Computationally expensive for 40+ features\n",
        "   - Risk of data leakage between train/test sets\n",
        "   - May introduce artificial relationships\n",
        "\n",
        "\u274c **Complete Case Deletion for All Missing**: Would lose too much valuable data given the extent of missingness\n",
        "\n",
        "\u274c **Dropping High-Missing Features**: Could lose important predictors; let feature selection decide\n",
        "\n",
        "#### **Validation of Approach**\n",
        "\n",
        "Our chosen strategy is validated by:\n",
        "1. **Preserving sample size**: Retains maximum information for modeling\n",
        "2. **Maintaining data integrity**: Country-specific imputation preserves realistic economic contexts\n",
        "3. **Standard practice**: Widely used in macroeconomic research and cross-country analyses\n",
        "4. **Robustness**: Feature selection and model validation will identify if imputation introduced issues\n",
        "\n",
        "#### **Limitations & Assumptions**\n",
        "\n",
        "- Assumes missingness is MAR (Missing At Random) rather than MNAR (Missing Not At Random)\n",
        "- May underestimate variance in imputed features\n",
        "- Does not account for structural reasons for missingness (e.g., data not collected in certain countries)\n",
        "- Imputed values treated as observed in subsequent analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Missing Value Handling Implementation\n",
        "print(\"\ud83d\udd27 IMPLEMENTING MISSING VALUE STRATEGY\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Store original shape for comparison\n",
        "original_shape = df.shape\n",
        "original_missing = df.isna().sum().sum()\n",
        "\n",
        "print(f\"\\n\ud83d\udcca Starting Dataset:\")\n",
        "print(f\"   Shape: {original_shape}\")\n",
        "print(f\"   Total missing values: {original_missing:,}\")\n",
        "\n",
        "# STEP 1: Remove rows with missing target variable\n",
        "print(f\"\\n1\ufe0f\u20e3 Removing rows with missing target variable ('{target_col}')...\")\n",
        "rows_before = len(df)\n",
        "df = df.dropna(subset=[target_col]).copy()\n",
        "rows_removed = rows_before - len(df)\n",
        "print(f\"   \u2713 Removed {rows_removed} rows ({rows_removed/rows_before*100:.1f}% of data)\")\n",
        "print(f\"   \u2713 Remaining: {len(df)} rows\")\n",
        "\n",
        "# STEP 2: Identify feature columns\n",
        "base_feature_cols = [col for col in df.columns if col not in ['country', 'date', 'year', target_col]]\n",
        "print(f\"\\n2\ufe0f\u20e3 Processing {len(base_feature_cols)} feature columns...\")\n",
        "\n",
        "# STEP 3: Apply within-country median imputation\n",
        "print(f\"\\n3\ufe0f\u20e3 Applying within-country median imputation...\")\n",
        "imputed_count = 0\n",
        "for col in base_feature_cols:\n",
        "    if pd.api.types.is_numeric_dtype(df[col]):\n",
        "        missing_before = df[col].isna().sum()\n",
        "        if missing_before > 0:\n",
        "            # Country-specific imputation\n",
        "            df[col] = df.groupby('country')[col].transform(lambda s: s.fillna(s.median()))\n",
        "            imputed_count += 1\n",
        "\n",
        "print(f\"   \u2713 Applied country-specific imputation to {imputed_count} features\")\n",
        "\n",
        "# STEP 4: Apply global median fallback\n",
        "print(f\"\\n4\ufe0f\u20e3 Applying global median fallback for remaining missing values...\")\n",
        "fallback_count = 0\n",
        "for col in base_feature_cols:\n",
        "    if pd.api.types.is_numeric_dtype(df[col]):\n",
        "        missing_before = df[col].isna().sum()\n",
        "        if missing_before > 0:\n",
        "            df[col] = df[col].fillna(df[col].median())\n",
        "            fallback_count += 1\n",
        "\n",
        "print(f\"   \u2713 Applied global median to {fallback_count} features\")\n",
        "\n",
        "# STEP 5: Final verification\n",
        "final_missing = df.isna().sum().sum()\n",
        "print(f\"\\n\u2705 MISSING VALUE HANDLING COMPLETE\")\n",
        "print(f\"\\n\ud83d\udcca Final Dataset:\")\n",
        "print(f\"   Shape: {df.shape}\")\n",
        "print(f\"   Rows retained: {len(df)/original_shape[0]*100:.1f}%\")\n",
        "print(f\"   Remaining missing values: {final_missing}\")\n",
        "print(f\"   Missing values handled: {original_missing - final_missing:,}\")\n",
        "\n",
        "if final_missing > 0:\n",
        "    print(f\"\\n\u26a0\ufe0f Warning: {final_missing} missing values remain (likely in non-numeric columns)\")\n",
        "    remaining_missing = df.isna().sum()[df.isna().sum() > 0]\n",
        "    print(f\"   Columns: {remaining_missing.to_dict()}\")\n",
        "else:\n",
        "    print(f\"\\n\u2713 All missing values successfully handled!\")\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Engineering\n",
        "\n",
        "Creating advanced features including:\n",
        "- Growth rates (year-over-year changes)\n",
        "- Economic structure indicators\n",
        "- Composite indices (Human Capital, Infrastructure)\n",
        "- Demographic ratios\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create engineered features\n",
        "print(\"\u2699\ufe0f Engineering advanced features...\\n\")\n",
        "engineered_df = df.copy()\n",
        "engineered_df = engineered_df.sort_values(['country', 'year']).reset_index(drop=True)\n",
        "\n",
        "feature_count = 0\n",
        "\n",
        "# 1. GROWTH RATES (Year-over-Year % change)\n",
        "print(\"1\ufe0f\u20e3 Creating growth rate features...\")\n",
        "growth_cols = [target_col, 'Labor force, total', 'Population', 'Exports (% of GDP)', 'FDI net inflows (% of GDP)']\n",
        "for col in growth_cols:\n",
        "    if col in engineered_df.columns:\n",
        "        new_col = f'{col} YoY %'\n",
        "        engineered_df[new_col] = (engineered_df.groupby('country')[col].pct_change().fillna(0) * 100)\n",
        "        feature_count += 1\n",
        "\n",
        "# 2. ECONOMIC STRUCTURE INDICATORS\n",
        "print(\"2\ufe0f\u20e3 Creating economic structure indicators...\")\n",
        "if 'Exports (% of GDP)' in engineered_df.columns and 'Imports (% of GDP)' in engineered_df.columns:\n",
        "    engineered_df['Trade Balance (% of GDP)'] = engineered_df['Exports (% of GDP)'] - engineered_df['Imports (% of GDP)']\n",
        "    feature_count += 1\n",
        "\n",
        "if 'Industry value added (% of GDP)' in engineered_df.columns and 'Services value added (% of GDP)' in engineered_df.columns:\n",
        "    engineered_df['Economic Diversification Index'] = engineered_df['Industry value added (% of GDP)'] + engineered_df['Services value added (% of GDP)']\n",
        "    feature_count += 1\n",
        "\n",
        "# 3. HUMAN CAPITAL COMPOSITE\n",
        "print(\"3\ufe0f\u20e3 Creating human capital composite...\")\n",
        "hc_components = []\n",
        "if 'Life expectancy at birth' in engineered_df.columns:\n",
        "    hc_components.append(engineered_df['Life expectancy at birth'] / 100)\n",
        "if 'Secondary school enrollment rate' in engineered_df.columns:\n",
        "    hc_components.append(engineered_df['Secondary school enrollment rate'] / 100)\n",
        "if 'Adult literacy rate' in engineered_df.columns:\n",
        "    hc_components.append(engineered_df['Adult literacy rate'] / 100)\n",
        "\n",
        "if hc_components:\n",
        "    engineered_df['Human Capital Index'] = sum(hc_components) / len(hc_components)\n",
        "    feature_count += 1\n",
        "\n",
        "# 4. INFRASTRUCTURE INDEX\n",
        "print(\"4\ufe0f\u20e3 Creating infrastructure index...\")\n",
        "infra_components = []\n",
        "if 'Internet users (% of population)' in engineered_df.columns:\n",
        "    infra_components.append(engineered_df['Internet users (% of population)'] / 100)\n",
        "if 'Electric power consumption (kWh per capita)' in engineered_df.columns:\n",
        "    infra_components.append(engineered_df['Electric power consumption (kWh per capita)'] / 10000)\n",
        "if 'Mobile subscriptions per 100 people' in engineered_df.columns:\n",
        "    infra_components.append(engineered_df['Mobile subscriptions per 100 people'] / 100)\n",
        "\n",
        "if infra_components:\n",
        "    engineered_df['Infrastructure Index'] = sum(infra_components) / len(infra_components)\n",
        "    feature_count += 1\n",
        "\n",
        "# 5. DEMOGRAPHIC INDICATORS\n",
        "print(\"5\ufe0f\u20e3 Creating demographic indicators...\")\n",
        "if 'Population ages 0-14 (% of total)' in engineered_df.columns and 'Population ages 65+ (% of total)' in engineered_df.columns:\n",
        "    engineered_df['Working Age Population (%)'] = 100 - (engineered_df['Population ages 0-14 (% of total)'] + \n",
        "                                                           engineered_df['Population ages 65+ (% of total)'])\n",
        "    feature_count += 1\n",
        "\n",
        "if 'Labor force, total' in engineered_df.columns and 'Population' in engineered_df.columns:\n",
        "    engineered_df['Labor Force Participation Rate (%)'] = (engineered_df['Labor force, total'] / engineered_df['Population']) * 100\n",
        "    feature_count += 1\n",
        "\n",
        "# 6. INVESTMENT & SPENDING RATIOS\n",
        "print(\"6\ufe0f\u20e3 Creating spending ratios...\")\n",
        "if 'Health expenditure (% of GDP)' in engineered_df.columns and 'Education expenditure (% of GDP)' in engineered_df.columns:\n",
        "    engineered_df['Health to Education Spending Ratio'] = (engineered_df['Health expenditure (% of GDP)'] / \n",
        "                                                            (engineered_df['Education expenditure (% of GDP)'] + 0.01))  # Avoid division by zero\n",
        "    feature_count += 1\n",
        "\n",
        "# 7. LOG TRANSFORMATIONS\n",
        "print(\"7\ufe0f\u20e3 Creating log transformations...\")\n",
        "log_cols = ['Population', 'Labor force, total', 'Energy use per capita', target_col]\n",
        "for col in log_cols:\n",
        "    if col in engineered_df.columns:\n",
        "        engineered_df[f'Log {col}'] = np.log1p(engineered_df[col])\n",
        "        feature_count += 1\n",
        "\n",
        "# 8. INTERACTION TERMS\n",
        "print(\"8\ufe0f\u20e3 Creating interaction terms...\")\n",
        "interactions = [\n",
        "    ('Life expectancy at birth', 'Unemployment rate (%)'),\n",
        "    ('Internet users (% of population)', 'Tertiary school enrollment rate'),\n",
        "    ('Urban population (% of total)', 'Services value added (% of GDP)')\n",
        "]\n",
        "\n",
        "for col1, col2 in interactions:\n",
        "    if col1 in engineered_df.columns and col2 in engineered_df.columns:\n",
        "        engineered_df[f'{col1} \u00d7 {col2}'] = engineered_df[col1] * engineered_df[col2]\n",
        "        feature_count += 1\n",
        "\n",
        "# 9. SQUARED TERMS (for non-linear relationships)\n",
        "print(\"9\ufe0f\u20e3 Creating squared terms...\")\n",
        "squared_cols = ['Life expectancy at birth', 'Inflation rate (CPI %)', 'Internet users (% of population)']\n",
        "for col in squared_cols:\n",
        "    if col in engineered_df.columns:\n",
        "        engineered_df[f'{col} Squared'] = engineered_df[col] ** 2\n",
        "        feature_count += 1\n",
        "\n",
        "# 10. LAGGED FEATURES (Previous year values)\n",
        "print(\"\ud83d\udd1f Creating lagged features...\")\n",
        "lag_cols = ['Inflation rate (CPI %)', 'FDI net inflows (% of GDP)', 'Trade (% of GDP)']\n",
        "for col in lag_cols:\n",
        "    if col in engineered_df.columns:\n",
        "        engineered_df[f'{col} Lag1'] = engineered_df.groupby('country')[col].shift(1)\n",
        "        engineered_df[f'{col} Lag1'] = engineered_df[f'{col} Lag1'].fillna(engineered_df[col])\n",
        "        feature_count += 1\n",
        "\n",
        "# 11. MOVING AVERAGES\n",
        "print(\"1\ufe0f\u20e31\ufe0f\u20e3 Creating moving averages...\")\n",
        "ma_cols = ['Inflation rate (CPI %)', 'Population growth (%)']\n",
        "for col in ma_cols:\n",
        "    if col in engineered_df.columns:\n",
        "        engineered_df[f'{col} MA3'] = engineered_df.groupby('country')[col].rolling(3, min_periods=1).mean().reset_index(0, drop=True)\n",
        "        feature_count += 1\n",
        "\n",
        "print(f\"\\n\u2705 Feature engineering complete!\")\n",
        "print(f\"\u2705 Created {feature_count} new features\")\n",
        "print(f\"\u2705 Total features now: {len([c for c in engineered_df.columns if c not in ['country', 'date', target_col]])}\")\n",
        "\n",
        "# Display sample of new features\n",
        "new_features = [col for col in engineered_df.columns if col not in df.columns]\n",
        "print(f\"\\n\ud83d\udccb Sample of new features ({len(new_features)} total):\")\n",
        "for feat in new_features[:15]:\n",
        "    print(f\"   \u2022 {feat}\")\n",
        "if len(new_features) > 15:\n",
        "    print(f\"   ... and {len(new_features) - 15} more\")\n",
        "\n",
        "display(engineered_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize key engineered features\n",
        "viz_features = [\n",
        "    'Human Capital Index',\n",
        "    'Infrastructure Index',\n",
        "    'Working Age Population (%)',\n",
        "    'Economic Diversification Index'\n",
        "]\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, feature in enumerate(viz_features):\n",
        "    if feature in engineered_df.columns:\n",
        "        # Distribution\n",
        "        axes[idx].hist(engineered_df[feature].dropna(), bins=25, edgecolor='black', alpha=0.7)\n",
        "        axes[idx].set_xlabel(feature)\n",
        "        axes[idx].set_ylabel('Frequency')\n",
        "        axes[idx].set_title(f'Distribution: {feature}')\n",
        "        \n",
        "        # Add statistics\n",
        "        mean_val = engineered_df[feature].mean()\n",
        "        median_val = engineered_df[feature].median()\n",
        "        axes[idx].axvline(mean_val, color='red', linestyle='--', label=f'Mean: {mean_val:.2f}')\n",
        "        axes[idx].axvline(median_val, color='green', linestyle='--', label=f'Median: {median_val:.2f}')\n",
        "        axes[idx].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. Modeling\n",
        "\n",
        "### Feature Selection\n",
        "\n",
        "With 60+ features, we'll use statistical methods to identify the most important predictors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features for selection\n",
        "model_df = engineered_df.copy()\n",
        "\n",
        "# Exclude metadata and target\n",
        "exclude_cols = ['country', 'date', target_col, f'Log {target_col}', f'{target_col} YoY %']\n",
        "all_feature_cols = [col for col in model_df.columns if col not in exclude_cols]\n",
        "\n",
        "# Select only numeric features and handle any remaining NaNs\n",
        "X_all = model_df[all_feature_cols].select_dtypes(include=[np.number])\n",
        "X_all = X_all.fillna(X_all.median())\n",
        "y_all = model_df[target_col].values\n",
        "\n",
        "print(f\"\ud83d\udcca Total features available: {X_all.shape[1]}\")\n",
        "print(f\"\ud83d\udcca Total observations: {X_all.shape[0]}\")\n",
        "print(f\"\\n\ud83d\udccb Feature list (first 20):\")\n",
        "for i, feat in enumerate(X_all.columns[:20], 1):\n",
        "    print(f\"   {i}. {feat}\")\n",
        "if len(X_all.columns) > 20:\n",
        "    print(f\"   ... and {len(X_all.columns) - 20} more\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Univariate feature selection\n",
        "print(\"\ud83d\udd0d Performing univariate feature selection...\\n\")\n",
        "\n",
        "k_best = SelectKBest(score_func=f_regression, k='all')\n",
        "k_best.fit(X_all, y_all)\n",
        "\n",
        "# Create feature importance dataframe\n",
        "feature_scores = pd.DataFrame({\n",
        "    'Feature': X_all.columns,\n",
        "    'F-Score': k_best.scores_,\n",
        "    'P-Value': k_best.pvalues_\n",
        "}).sort_values('F-Score', ascending=False)\n",
        "\n",
        "print(\"\ud83c\udfc6 Top 30 Features by F-Score:\\n\")\n",
        "display(feature_scores.head(30))\n",
        "\n",
        "# Visualize top features\n",
        "plt.figure(figsize=(12, 8))\n",
        "top_n = 25\n",
        "top_features_viz = feature_scores.head(top_n)\n",
        "plt.barh(range(len(top_features_viz)), top_features_viz['F-Score'])\n",
        "plt.yticks(range(len(top_features_viz)), top_features_viz['Feature'])\n",
        "plt.xlabel('F-Score')\n",
        "plt.title(f'Top {top_n} Features by Univariate F-Score', fontsize=14, fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Select top features for modeling\n",
        "n_features_to_keep = 40  # Balance between information and overfitting\n",
        "top_features = feature_scores.head(n_features_to_keep)['Feature'].tolist()\n",
        "print(f\"\\n\u2705 Selected top {n_features_to_keep} features for modeling\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fit Models\n",
        "\n",
        "We'll train multiple regression models:\n",
        "- Linear Regression (baseline)\n",
        "- Ridge and Lasso (regularized)\n",
        "- Random Forest (ensemble)\n",
        "- Gradient Boosting (ensemble)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Country-level split\n",
        "countries = model_df['country'].unique()\n",
        "train_countries, test_countries = train_test_split(countries, test_size=0.25, random_state=42)\n",
        "\n",
        "train_mask = model_df['country'].isin(train_countries)\n",
        "test_mask = model_df['country'].isin(test_countries)\n",
        "\n",
        "# Use selected features\n",
        "X_selected = X_all[top_features]\n",
        "\n",
        "X_train = X_selected[train_mask]\n",
        "X_test = X_selected[test_mask]\n",
        "y_train = y_all[train_mask]\n",
        "y_test = y_all[test_mask]\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"\ud83c\udfaf Training Configuration:\\n\")\n",
        "print(f\"Train countries ({len(train_countries)}): {', '.join(sorted(train_countries))}\")\n",
        "print(f\"\\nTest countries ({len(test_countries)}): {', '.join(sorted(test_countries))}\")\n",
        "print(f\"\\nDataset sizes:\")\n",
        "print(f\"  \u2022 Train: {X_train.shape[0]} samples\")\n",
        "print(f\"  \u2022 Test: {X_test.shape[0]} samples\")\n",
        "print(f\"  \u2022 Features: {X_train.shape[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train multiple models\n",
        "models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Ridge (\u03b1=10)': Ridge(alpha=10.0),\n",
        "    'Lasso (\u03b1=100)': Lasso(alpha=100.0, max_iter=5000),\n",
        "    'Random Forest': RandomForestRegressor(\n",
        "        n_estimators=300, \n",
        "        max_depth=15, \n",
        "        min_samples_leaf=2,\n",
        "        max_features='sqrt',\n",
        "        random_state=42, \n",
        "        n_jobs=-1\n",
        "    ),\n",
        "    'Gradient Boosting': GradientBoostingRegressor(\n",
        "        n_estimators=300, \n",
        "        max_depth=6, \n",
        "        learning_rate=0.05,\n",
        "        subsample=0.8,\n",
        "        random_state=42\n",
        "    )\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "print(\"\ud83d\ude80 Training models with enhanced feature set...\\n\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n\ud83d\udcca Training {name}...\")\n",
        "    \n",
        "    # Train\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    \n",
        "    # Predictions\n",
        "    y_train_pred = model.predict(X_train_scaled)\n",
        "    y_test_pred = model.predict(X_test_scaled)\n",
        "    \n",
        "    # Metrics\n",
        "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "    train_r2 = r2_score(y_train, y_train_pred)\n",
        "    test_r2 = r2_score(y_test, y_test_pred)\n",
        "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "    test_mape = np.mean(np.abs((y_test - y_test_pred) / y_test)) * 100\n",
        "    \n",
        "    # Cross-validation\n",
        "    cv_scores = cross_val_score(model, X_train_scaled, y_train, \n",
        "                                cv=5, scoring='r2', n_jobs=-1)\n",
        "    \n",
        "    results[name] = {\n",
        "        'model': model,\n",
        "        'train_rmse': train_rmse,\n",
        "        'test_rmse': test_rmse,\n",
        "        'train_r2': train_r2,\n",
        "        'test_r2': test_r2,\n",
        "        'test_mae': test_mae,\n",
        "        'test_mape': test_mape,\n",
        "        'cv_r2_mean': cv_scores.mean(),\n",
        "        'cv_r2_std': cv_scores.std(),\n",
        "        'predictions': y_test_pred\n",
        "    }\n",
        "    \n",
        "    print(f\"   Train RMSE: ${train_rmse:,.0f} | Test RMSE: ${test_rmse:,.0f}\")\n",
        "    print(f\"   Train R\u00b2: {train_r2:.4f} | Test R\u00b2: {test_r2:.4f}\")\n",
        "    print(f\"   Test MAE: ${test_mae:,.0f} | Test MAPE: {test_mape:.2f}%\")\n",
        "    print(f\"   CV R\u00b2 (5-fold): {cv_scores.mean():.4f} \u00b1 {cv_scores.std():.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\u2705 All models trained successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validate Models\n",
        "\n",
        "Compare model performance using multiple metrics:\n",
        "- RMSE (Root Mean Squared Error)\n",
        "- R\u00b2 Score (Coefficient of Determination)\n",
        "- MAE (Mean Absolute Error)\n",
        "- MAPE (Mean Absolute Percentage Error)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive model comparison\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': results.keys(),\n",
        "    'Train RMSE': [f\"${r['train_rmse']:,.0f}\" for r in results.values()],\n",
        "    'Test RMSE': [f\"${r['test_rmse']:,.0f}\" for r in results.values()],\n",
        "    'Train R\u00b2': [f\"{r['train_r2']:.4f}\" for r in results.values()],\n",
        "    'Test R\u00b2': [f\"{r['test_r2']:.4f}\" for r in results.values()],\n",
        "    'Test MAE': [f\"${r['test_mae']:,.0f}\" for r in results.values()],\n",
        "    'Test MAPE': [f\"{r['test_mape']:.2f}%\" for r in results.values()],\n",
        "    'CV R\u00b2 (mean\u00b1std)': [f\"{r['cv_r2_mean']:.4f}\u00b1{r['cv_r2_std']:.4f}\" for r in results.values()]\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"\ud83d\udcca COMPREHENSIVE MODEL COMPARISON\")\n",
        "print(\"=\"*100)\n",
        "display(comparison_df)\n",
        "\n",
        "# Find best model\n",
        "best_model_name = max(results.keys(), key=lambda k: results[k]['test_r2'])\n",
        "best_results = results[best_model_name]\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(f\"\ud83c\udfc6 BEST MODEL: {best_model_name}\")\n",
        "print(\"=\"*100)\n",
        "print(f\"Test R\u00b2: {best_results['test_r2']:.4f}\")\n",
        "print(f\"Test RMSE: ${best_results['test_rmse']:,.0f}\")\n",
        "print(f\"Test MAE: ${best_results['test_mae']:,.0f}\")\n",
        "print(f\"Test MAPE: {best_results['test_mape']:.2f}%\")\n",
        "print(f\"CV R\u00b2: {best_results['cv_r2_mean']:.4f} \u00b1 {best_results['cv_r2_std']:.4f}\")\n",
        "print(\"=\"*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visual model comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "model_names = list(results.keys())\n",
        "x_pos = np.arange(len(model_names))\n",
        "width = 0.35\n",
        "\n",
        "# 1. RMSE comparison\n",
        "train_rmses = [results[m]['train_rmse'] for m in model_names]\n",
        "test_rmses = [results[m]['test_rmse'] for m in model_names]\n",
        "axes[0, 0].bar(x_pos - width/2, train_rmses, width, label='Train RMSE', alpha=0.8)\n",
        "axes[0, 0].bar(x_pos + width/2, test_rmses, width, label='Test RMSE', alpha=0.8)\n",
        "axes[0, 0].set_xlabel('Model')\n",
        "axes[0, 0].set_ylabel('RMSE ($)')\n",
        "axes[0, 0].set_title('RMSE Comparison')\n",
        "axes[0, 0].set_xticks(x_pos)\n",
        "axes[0, 0].set_xticklabels(model_names, rotation=20, ha='right')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 2. R\u00b2 comparison\n",
        "train_r2s = [results[m]['train_r2'] for m in model_names]\n",
        "test_r2s = [results[m]['test_r2'] for m in model_names]\n",
        "axes[0, 1].bar(x_pos - width/2, train_r2s, width, label='Train R\u00b2', alpha=0.8)\n",
        "axes[0, 1].bar(x_pos + width/2, test_r2s, width, label='Test R\u00b2', alpha=0.8)\n",
        "axes[0, 1].set_xlabel('Model')\n",
        "axes[0, 1].set_ylabel('R\u00b2 Score')\n",
        "axes[0, 1].set_title('R\u00b2 Score Comparison')\n",
        "axes[0, 1].set_xticks(x_pos)\n",
        "axes[0, 1].set_xticklabels(model_names, rotation=20, ha='right')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].set_ylim([0, 1])\n",
        "axes[0, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 3. MAE comparison\n",
        "test_maes = [results[m]['test_mae'] for m in model_names]\n",
        "axes[1, 0].bar(x_pos, test_maes, alpha=0.8, color='coral')\n",
        "axes[1, 0].set_xlabel('Model')\n",
        "axes[1, 0].set_ylabel('MAE ($)')\n",
        "axes[1, 0].set_title('Mean Absolute Error')\n",
        "axes[1, 0].set_xticks(x_pos)\n",
        "axes[1, 0].set_xticklabels(model_names, rotation=20, ha='right')\n",
        "axes[1, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 4. MAPE comparison\n",
        "test_mapes = [results[m]['test_mape'] for m in model_names]\n",
        "axes[1, 1].bar(x_pos, test_mapes, alpha=0.8, color='lightgreen')\n",
        "axes[1, 1].set_xlabel('Model')\n",
        "axes[1, 1].set_ylabel('MAPE (%)')\n",
        "axes[1, 1].set_title('Mean Absolute Percentage Error')\n",
        "axes[1, 1].set_xticks(x_pos)\n",
        "axes[1, 1].set_xticklabels(model_names, rotation=20, ha='right')\n",
        "axes[1, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. Evaluation\n",
        "\n",
        "Now we'll answer our research questions using the insights from our analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 1: What are the strongest predictors of GDP per capita among technology, education, and healthcare indicators?\n",
        "\n",
        "#### Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze top predictors in technology, education, and healthcare categories\n",
        "print(\"\ud83d\udd0d Analyzing Feature Importance for Question 1\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Get feature importance from best model\n",
        "if best_tree_model:\n",
        "    model = results[best_tree_model]['model']\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'Feature': top_features,\n",
        "        'Importance': model.feature_importances_\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "    \n",
        "    # Filter for technology, education, and healthcare indicators\n",
        "    tech_keywords = ['Internet', 'Electric', 'power', 'Technology', 'ICT', 'Mobile']\n",
        "    edu_keywords = ['Education', 'School', 'enrollment', 'literacy', 'Secondary', 'Tertiary']\n",
        "    health_keywords = ['Health', 'Life expectancy', 'mortality', 'Healthcare']\n",
        "    \n",
        "    tech_features = feature_importance[feature_importance['Feature'].str.contains('|'.join(tech_keywords), case=False, na=False)]\n",
        "    edu_features = feature_importance[feature_importance['Feature'].str.contains('|'.join(edu_keywords), case=False, na=False)]\n",
        "    health_features = feature_importance[feature_importance['Feature'].str.contains('|'.join(health_keywords), case=False, na=False)]\n",
        "    \n",
        "    print(\"\\n\ud83d\udce1 TOP TECHNOLOGY INDICATORS:\")\n",
        "    print(tech_features.head(5).to_string(index=False))\n",
        "    \n",
        "    print(\"\\n\ud83c\udf93 TOP EDUCATION INDICATORS:\")\n",
        "    print(edu_features.head(5).to_string(index=False))\n",
        "    \n",
        "    print(\"\\n\ud83c\udfe5 TOP HEALTHCARE INDICATORS:\")\n",
        "    print(health_features.head(5).to_string(index=False))\n",
        "else:\n",
        "    print(\"Using correlation analysis instead...\")\n",
        "    # Show top correlations for these categories\n",
        "    display(correlations.head(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the relationship between top predictors and GDP\n",
        "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "# Select key features from each category\n",
        "key_features = [\n",
        "    'Internet users (% of population)',\n",
        "    'Electric power consumption (kWh per capita)',\n",
        "    'Secondary school enrollment rate',\n",
        "    'Tertiary school enrollment rate',\n",
        "    'Life expectancy at birth',\n",
        "    'Health expenditure (% of GDP)'\n",
        "]\n",
        "\n",
        "for idx, feature in enumerate(key_features):\n",
        "    if feature in df.columns:\n",
        "        axes[idx].scatter(df[feature], df[target_col], alpha=0.5, s=30, c='steelblue')\n",
        "        axes[idx].set_xlabel(feature, fontsize=10)\n",
        "        axes[idx].set_ylabel('GDP per Capita ($)', fontsize=10)\n",
        "        axes[idx].set_title(feature, fontsize=11, fontweight='bold')\n",
        "        \n",
        "        # Add correlation\n",
        "        if feature in correlations.index:\n",
        "            corr_val = correlations[feature]\n",
        "            axes[idx].text(0.05, 0.95, f'r = {corr_val:.3f}', \n",
        "                          transform=axes[idx].transAxes, \n",
        "                          bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
        "                          verticalalignment='top')\n",
        "        \n",
        "        axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('question1_predictors.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udcca Visualization saved as 'question1_predictors.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Brief Explanation\n",
        "\n",
        "**Technology indicators** show the strongest relationship with GDP per capita:\n",
        "- **Internet penetration** and **electric power consumption** are among the top predictors, indicating that technological infrastructure is crucial for economic development\n",
        "- These metrics reflect both the availability of modern infrastructure and the productive capacity of an economy\n",
        "\n",
        "**Education indicators** demonstrate significant but more moderate effects:\n",
        "- **Secondary and tertiary enrollment rates** show positive correlations with GDP\n",
        "- Higher education levels appear to be both a cause and consequence of economic prosperity\n",
        "\n",
        "**Healthcare indicators** like **life expectancy** are strongly correlated but may represent outcomes rather than drivers:\n",
        "- While important for human development, healthcare spending shows weaker predictive power than technology metrics\n",
        "- Life expectancy improvements often follow economic growth rather than precede it\n",
        "\n",
        "**Key Insight:** Technology and infrastructure development appear to be the most powerful predictors of economic prosperity, followed by education investments.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Question 2: How do trade openness and economic structure relate to economic prosperity?\n",
        "\n",
        "#### Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze trade and economic structure indicators\n",
        "print(\"\ud83d\udd0d Analyzing Trade & Economic Structure for Question 2\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Select trade-related features\n",
        "trade_features = [\n",
        "    'Trade (% of GDP)',\n",
        "    'Exports (% of GDP)',\n",
        "    'Imports (% of GDP)',\n",
        "    'FDI net inflows (% of GDP)'\n",
        "]\n",
        "\n",
        "# Calculate correlations\n",
        "print(\"\\n\ud83d\udcca CORRELATIONS WITH GDP PER CAPITA:\\n\")\n",
        "for feature in trade_features:\n",
        "    if feature in correlations.index:\n",
        "        corr = correlations[feature]\n",
        "        print(f\"{feature:40s}: {corr:+.4f}\")\n",
        "\n",
        "# Calculate trade balance if possible\n",
        "if 'Exports (% of GDP)' in df.columns and 'Imports (% of GDP)' in df.columns:\n",
        "    df['Trade Balance (% of GDP)'] = df['Exports (% of GDP)'] - df['Imports (% of GDP)']\n",
        "    trade_balance_corr = df['Trade Balance (% of GDP)'].corr(df[target_col])\n",
        "    print(f\"{'Trade Balance (% of GDP)':40s}: {trade_balance_corr:+.4f}\")\n",
        "\n",
        "# Group analysis\n",
        "print(\"\\n\\n\ud83d\udcc8 TRADE OPENNESS ANALYSIS:\\n\")\n",
        "if 'Trade (% of GDP)' in df.columns:\n",
        "    df['Trade_Category'] = pd.qcut(df['Trade (% of GDP)'], q=4, \n",
        "                                     labels=['Low', 'Medium-Low', 'Medium-High', 'High'])\n",
        "    trade_analysis = df.groupby('Trade_Category')[target_col].agg(['mean', 'median', 'count'])\n",
        "    trade_analysis.columns = ['Avg GDP per Capita', 'Median GDP per Capita', 'N Countries']\n",
        "    display(trade_analysis)\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Trade openness shows a complex relationship with economic prosperity.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize trade relationships\n",
        "fig = plt.figure(figsize=(16, 10))\n",
        "gs = fig.add_gridspec(2, 3, hspace=0.3, wspace=0.3)\n",
        "\n",
        "# Plot 1: Trade openness vs GDP\n",
        "ax1 = fig.add_subplot(gs[0, 0])\n",
        "if 'Trade (% of GDP)' in df.columns:\n",
        "    ax1.scatter(df['Trade (% of GDP)'], df[target_col], alpha=0.5, c='coral', s=40)\n",
        "    ax1.set_xlabel('Trade (% of GDP)', fontsize=11)\n",
        "    ax1.set_ylabel('GDP per Capita ($)', fontsize=11)\n",
        "    ax1.set_title('Trade Openness vs GDP per Capita', fontsize=12, fontweight='bold')\n",
        "    if 'Trade (% of GDP)' in correlations.index:\n",
        "        ax1.text(0.05, 0.95, f\"r = {correlations['Trade (% of GDP)']:.3f}\",\n",
        "                transform=ax1.transAxes, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
        "                verticalalignment='top')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Exports vs GDP\n",
        "ax2 = fig.add_subplot(gs[0, 1])\n",
        "if 'Exports (% of GDP)' in df.columns:\n",
        "    ax2.scatter(df['Exports (% of GDP)'], df[target_col], alpha=0.5, c='green', s=40)\n",
        "    ax2.set_xlabel('Exports (% of GDP)', fontsize=11)\n",
        "    ax2.set_ylabel('GDP per Capita ($)', fontsize=11)\n",
        "    ax2.set_title('Export Intensity vs GDP per Capita', fontsize=12, fontweight='bold')\n",
        "    if 'Exports (% of GDP)' in correlations.index:\n",
        "        ax2.text(0.05, 0.95, f\"r = {correlations['Exports (% of GDP)']:.3f}\",\n",
        "                transform=ax2.transAxes, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
        "                verticalalignment='top')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: FDI vs GDP\n",
        "ax3 = fig.add_subplot(gs[0, 2])\n",
        "if 'FDI net inflows (% of GDP)' in df.columns:\n",
        "    ax3.scatter(df['FDI net inflows (% of GDP)'], df[target_col], alpha=0.5, c='purple', s=40)\n",
        "    ax3.set_xlabel('FDI Net Inflows (% of GDP)', fontsize=11)\n",
        "    ax3.set_ylabel('GDP per Capita ($)', fontsize=11)\n",
        "    ax3.set_title('Foreign Investment vs GDP per Capita', fontsize=12, fontweight='bold')\n",
        "    if 'FDI net inflows (% of GDP)' in correlations.index:\n",
        "        ax3.text(0.05, 0.95, f\"r = {correlations['FDI net inflows (% of GDP)']:.3f}\",\n",
        "                transform=ax3.transAxes, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
        "                verticalalignment='top')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Trade category box plot\n",
        "ax4 = fig.add_subplot(gs[1, :])\n",
        "if 'Trade_Category' in df.columns:\n",
        "    df.boxplot(column=target_col, by='Trade_Category', ax=ax4)\n",
        "    ax4.set_xlabel('Trade Openness Category', fontsize=11)\n",
        "    ax4.set_ylabel('GDP per Capita ($)', fontsize=11)\n",
        "    ax4.set_title('GDP Distribution by Trade Openness Level', fontsize=12, fontweight='bold')\n",
        "    plt.suptitle('')  # Remove default title\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.savefig('question2_trade_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udcca Visualization saved as 'question2_trade_analysis.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Brief Explanation\n",
        "\n",
        "**Trade openness shows a nuanced relationship with economic prosperity:**\n",
        "\n",
        "1. **Overall trade volume** (imports + exports as % of GDP) shows a positive but moderate correlation with GDP per capita\n",
        "   - Countries with very high trade openness tend to be either small economies heavily dependent on trade (like Singapore) or major manufacturing exporters\n",
        "   - The relationship is not strictly linear - some closed economies can still achieve high GDP through domestic markets\n",
        "\n",
        "2. **Export intensity** demonstrates a stronger positive relationship:\n",
        "   - Countries that export a higher percentage of GDP tend to have higher GDP per capita\n",
        "   - This suggests that export-oriented growth strategies can be effective\n",
        "   - However, the type and sophistication of exports matter more than volume alone\n",
        "\n",
        "3. **Foreign Direct Investment (FDI)** shows variable patterns:\n",
        "   - The correlation is weaker and more scattered than trade metrics\n",
        "   - Very high FDI inflows can indicate either attractive investment destinations or economies in transition\n",
        "   - Stable, developed economies may have lower FDI relative to GDP\n",
        "\n",
        "**Key Insight:** Trade openness and export capacity are associated with higher GDP per capita, but the quality and composition of trade matter more than raw volume. Economic integration appears beneficial but not sufficient for prosperity on its own.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Question 3: Which countries outperform or underperform relative to model predictions?\n",
        "\n",
        "#### Analysis\n",
        "\n",
        "For this analysis, we'll use the **full dataset** (all countries, both train and test) to get predictions and identify outliers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate predictions for ALL countries using the best model\n",
        "print(\"\ud83d\udd0d Analyzing Model Performance Across ALL Countries\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Use the best model to predict on all data\n",
        "print(f\"\\n\ud83d\udcca Using best model: {best_model_name}\")\n",
        "print(f\"   Model performance: R\u00b2 = {results[best_model_name]['test_r2']:.4f}\")\n",
        "\n",
        "# Get predictions for ALL data (not just test set)\n",
        "X_all_selected = X_all[top_features]\n",
        "X_all_scaled = scaler.transform(X_all_selected)\n",
        "y_all_pred = results[best_model_name]['model'].predict(X_all_scaled)\n",
        "\n",
        "# Create comprehensive results dataframe\n",
        "all_results = model_df.copy()\n",
        "all_results['prediction'] = y_all_pred\n",
        "all_results['error'] = all_results['prediction'] - all_results[target_col]\n",
        "all_results['abs_error'] = np.abs(all_results['error'])\n",
        "all_results['pct_error'] = (all_results['error'] / all_results[target_col]) * 100\n",
        "\n",
        "print(f\"\\n\u2713 Generated predictions for {len(all_results)} observations\")\n",
        "print(f\"\u2713 Covering {all_results['country'].nunique()} countries\")\n",
        "print(f\"\u2713 Time period: {all_results['year'].min()}-{all_results['year'].max()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate by country to identify systematic over/underperformers\n",
        "country_performance = all_results.groupby('country').agg({\n",
        "    target_col: ['mean', 'std'],\n",
        "    'prediction': 'mean',\n",
        "    'error': 'mean',\n",
        "    'abs_error': 'mean',\n",
        "    'pct_error': 'mean',\n",
        "    'year': 'count'\n",
        "}).round(2)\n",
        "\n",
        "# Flatten column names\n",
        "country_performance.columns = ['avg_actual_gdp', 'std_gdp', 'avg_predicted_gdp', \n",
        "                                'avg_error', 'avg_abs_error', 'avg_pct_error', 'n_observations']\n",
        "\n",
        "# Sort by error (negative = overperforming, positive = underperforming)\n",
        "country_performance = country_performance.sort_values('avg_error')\n",
        "\n",
        "print(\"\\n\ud83d\udcca COUNTRY-LEVEL PERFORMANCE SUMMARY\\n\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nNote: Negative error = Country GDP HIGHER than predicted (Overperformer)\")\n",
        "print(f\"      Positive error = Country GDP LOWER than predicted (Underperformer)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"\\n\ud83c\udf1f TOP 5 OVERPERFORMING COUNTRIES (GDP Higher than Predicted):\\n\")\n",
        "overperformers = country_performance.head(5)\n",
        "display(overperformers)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"\\n\u26a0\ufe0f TOP 5 UNDERPERFORMING COUNTRIES (GDP Lower than Predicted):\\n\")\n",
        "underperformers = country_performance.tail(5)\n",
        "display(underperformers)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"\\n\ud83c\udfaf MOST ACCURATE PREDICTIONS (Smallest Absolute Error):\\n\")\n",
        "accurate_predictions = country_performance.nsmallest(5, 'avg_abs_error')\n",
        "display(accurate_predictions)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"\\n\ud83d\udcc8 ALL COUNTRIES RANKED BY PERFORMANCE:\\n\")\n",
        "display(country_performance[['avg_actual_gdp', 'avg_predicted_gdp', 'avg_error', 'avg_pct_error', 'n_observations']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistical analysis of prediction errors\n",
        "print(\"\\n\ud83d\udcca STATISTICAL ANALYSIS OF PREDICTION ERRORS\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Overall error statistics\n",
        "print(\"\\n1\ufe0f\u20e3 Overall Error Statistics (All Countries):\")\n",
        "print(f\"   Mean Error: ${all_results['error'].mean():,.2f}\")\n",
        "print(f\"   Median Error: ${all_results['error'].median():,.2f}\")\n",
        "print(f\"   Std Dev of Error: ${all_results['error'].std():,.2f}\")\n",
        "print(f\"   Mean Absolute Error: ${all_results['abs_error'].mean():,.2f}\")\n",
        "print(f\"   Mean Absolute Percentage Error: {all_results['pct_error'].abs().mean():.2f}%\")\n",
        "\n",
        "# Identify significant outliers (beyond 1.5 IQR)\n",
        "Q1 = country_performance['avg_error'].quantile(0.25)\n",
        "Q3 = country_performance['avg_error'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "outlier_countries = country_performance[\n",
        "    (country_performance['avg_error'] < lower_bound) | \n",
        "    (country_performance['avg_error'] > upper_bound)\n",
        "]\n",
        "\n",
        "print(f\"\\n2\ufe0f\u20e3 Outlier Detection (1.5\u00d7IQR method):\")\n",
        "print(f\"   IQR Range: [{Q1:,.2f}, {Q3:,.2f}]\")\n",
        "print(f\"   Outlier Bounds: [{lower_bound:,.2f}, {upper_bound:,.2f}]\")\n",
        "print(f\"   Number of outlier countries: {len(outlier_countries)}\")\n",
        "if len(outlier_countries) > 0:\n",
        "    print(f\"\\n   Outlier countries:\")\n",
        "    for country in outlier_countries.index:\n",
        "        error = outlier_countries.loc[country, 'avg_error']\n",
        "        status = \"OVERPERFORMER\" if error < 0 else \"UNDERPERFORMER\"\n",
        "        print(f\"      \u2022 {country}: ${error:,.2f} ({status})\")\n",
        "\n",
        "# Correlation between actual GDP and prediction error\n",
        "error_gdp_corr = all_results['error'].corr(all_results[target_col])\n",
        "print(f\"\\n3\ufe0f\u20e3 Relationship Analysis:\")\n",
        "print(f\"   Correlation (Error vs Actual GDP): {error_gdp_corr:.4f}\")\n",
        "if abs(error_gdp_corr) > 0.3:\n",
        "    interpretation = \"Errors are systematically related to GDP level\"\n",
        "    if error_gdp_corr > 0:\n",
        "        interpretation += \" (model underestimates high-GDP countries)\"\n",
        "    else:\n",
        "        interpretation += \" (model overestimates high-GDP countries)\"\n",
        "else:\n",
        "    interpretation = \"Errors are relatively independent of GDP level (good!)\"\n",
        "print(f\"   Interpretation: {interpretation}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive visualization of country-level performance\n",
        "fig = plt.figure(figsize=(18, 12))\n",
        "gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
        "\n",
        "# Plot 1: Country ranking by average error\n",
        "ax1 = fig.add_subplot(gs[0, :])\n",
        "country_sorted = country_performance.sort_values('avg_error')\n",
        "colors = ['green' if x < 0 else 'red' for x in country_sorted['avg_error']]\n",
        "y_pos = np.arange(len(country_sorted))\n",
        "ax1.barh(y_pos, country_sorted['avg_error'], color=colors, alpha=0.7)\n",
        "ax1.set_yticks(y_pos)\n",
        "ax1.set_yticklabels(country_sorted.index, fontsize=9)\n",
        "ax1.set_xlabel('Average Prediction Error ($)', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('Country Performance: Overperformers (Green) vs Underperformers (Red)', \n",
        "              fontsize=14, fontweight='bold', pad=20)\n",
        "ax1.axvline(x=0, color='black', linestyle='--', linewidth=2)\n",
        "ax1.grid(axis='x', alpha=0.3)\n",
        "ax1.text(0.02, 0.98, '\u2190 GDP HIGHER than predicted', \n",
        "        transform=ax1.transAxes, fontsize=11, color='green', weight='bold',\n",
        "        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "ax1.text(0.98, 0.98, 'GDP LOWER than predicted \u2192', \n",
        "        transform=ax1.transAxes, fontsize=11, color='red', weight='bold',\n",
        "        verticalalignment='top', horizontalalignment='right',\n",
        "        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "# Plot 2: Actual vs Predicted scatter (all countries)\n",
        "ax2 = fig.add_subplot(gs[1, 0])\n",
        "ax2.scatter(country_performance['avg_actual_gdp'], \n",
        "           country_performance['avg_predicted_gdp'], \n",
        "           s=150, alpha=0.6, c='steelblue', edgecolors='black', linewidth=0.5)\n",
        "\n",
        "# Annotate all countries\n",
        "for country in country_performance.index:\n",
        "    ax2.annotate(country, \n",
        "                (country_performance.loc[country, 'avg_actual_gdp'], \n",
        "                 country_performance.loc[country, 'avg_predicted_gdp']),\n",
        "                fontsize=8, alpha=0.7, xytext=(3, 3), \n",
        "                textcoords='offset points')\n",
        "\n",
        "# Perfect prediction line\n",
        "lims = [min(country_performance['avg_actual_gdp'].min(), country_performance['avg_predicted_gdp'].min()) * 0.95,\n",
        "        max(country_performance['avg_actual_gdp'].max(), country_performance['avg_predicted_gdp'].max()) * 1.05]\n",
        "ax2.plot(lims, lims, 'r--', linewidth=2, label='Perfect Prediction', alpha=0.7)\n",
        "ax2.set_xlabel('Actual Average GDP per Capita ($)', fontsize=11, fontweight='bold')\n",
        "ax2.set_ylabel('Predicted Average GDP per Capita ($)', fontsize=11, fontweight='bold')\n",
        "ax2.set_title('All Countries: Actual vs Predicted GDP', fontsize=12, fontweight='bold')\n",
        "ax2.legend(fontsize=10)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Distribution of errors\n",
        "ax3 = fig.add_subplot(gs[1, 1])\n",
        "ax3.hist(country_performance['avg_error'], bins=15, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "ax3.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
        "ax3.axvline(x=country_performance['avg_error'].mean(), color='green', linestyle='--', \n",
        "           linewidth=2, label=f'Mean Error: ${country_performance[\"avg_error\"].mean():,.0f}')\n",
        "ax3.set_xlabel('Average Prediction Error ($)', fontsize=11, fontweight='bold')\n",
        "ax3.set_ylabel('Number of Countries', fontsize=11, fontweight='bold')\n",
        "ax3.set_title('Distribution of Country-Level Prediction Errors', fontsize=12, fontweight='bold')\n",
        "ax3.legend(fontsize=9)\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Absolute error by GDP level\n",
        "ax4 = fig.add_subplot(gs[2, 0])\n",
        "ax4.scatter(country_performance['avg_actual_gdp'], \n",
        "           country_performance['avg_abs_error'],\n",
        "           s=150, alpha=0.6, c='coral', edgecolors='black', linewidth=0.5)\n",
        "for country in country_performance.index:\n",
        "    ax4.annotate(country, \n",
        "                (country_performance.loc[country, 'avg_actual_gdp'], \n",
        "                 country_performance.loc[country, 'avg_abs_error']),\n",
        "                fontsize=8, alpha=0.7, xytext=(3, 3), \n",
        "                textcoords='offset points')\n",
        "ax4.set_xlabel('Actual Average GDP per Capita ($)', fontsize=11, fontweight='bold')\n",
        "ax4.set_ylabel('Average Absolute Error ($)', fontsize=11, fontweight='bold')\n",
        "ax4.set_title('Prediction Accuracy vs GDP Level', fontsize=12, fontweight='bold')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 5: Percentage error\n",
        "ax5 = fig.add_subplot(gs[2, 1])\n",
        "country_pct_sorted = country_performance.sort_values('avg_pct_error')\n",
        "colors_pct = ['green' if x < 0 else 'red' for x in country_pct_sorted['avg_pct_error']]\n",
        "ax5.barh(range(len(country_pct_sorted)), country_pct_sorted['avg_pct_error'], \n",
        "        color=colors_pct, alpha=0.7)\n",
        "ax5.set_yticks(range(len(country_pct_sorted)))\n",
        "ax5.set_yticklabels(country_pct_sorted.index, fontsize=9)\n",
        "ax5.set_xlabel('Average Percentage Error (%)', fontsize=11, fontweight='bold')\n",
        "ax5.set_title('Percentage Prediction Error by Country', fontsize=12, fontweight='bold')\n",
        "ax5.axvline(x=0, color='black', linestyle='--', linewidth=2)\n",
        "ax5.grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.savefig('question3_all_countries_performance.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udcca Visualization saved as 'question3_all_countries_performance.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Brief Explanation\n",
        "\n",
        "**Analysis across ALL countries** (not just test set) reveals important insights about model performance and economic patterns:\n",
        "\n",
        "#### **Overperforming Countries** (Higher GDP than predicted) \ud83c\udf1f\n",
        "\n",
        "These nations achieve greater economic prosperity than their measured indicators suggest. Common characteristics:\n",
        "\n",
        "1. **Strong Institutional Quality**\n",
        "   - Effective governance and rule of law not fully captured by our metrics\n",
        "   - Low corruption and transparent business environments\n",
        "   - Strong property rights protection\n",
        "\n",
        "2. **Strategic Economic Positioning**\n",
        "   - Geographic advantages (trade hubs, financial centers)\n",
        "   - Specialized high-value sectors (finance, technology, pharmaceuticals)\n",
        "   - Strong international partnerships and treaties\n",
        "\n",
        "3. **Effective Policy Implementation**\n",
        "   - Smart industrial policies and economic planning\n",
        "   - Investment in key strategic sectors\n",
        "   - Effective use of resources and infrastructure\n",
        "\n",
        "4. **Unmeasured Human Capital**\n",
        "   - Strong work ethic and productivity culture\n",
        "   - High social trust and cooperation\n",
        "   - Entrepreneurial ecosystem\n",
        "\n",
        "#### **Underperforming Countries** (Lower GDP than predicted) \u26a0\ufe0f\n",
        "\n",
        "These nations have solid fundamentals but achieve lower economic outcomes. Possible explanations:\n",
        "\n",
        "1. **Governance and Institutional Challenges**\n",
        "   - Political instability or policy uncertainty\n",
        "   - Corruption or inefficient bureaucracy\n",
        "   - Weak rule of law\n",
        "\n",
        "2. **Structural Economic Issues**\n",
        "   - Resource curse (over-dependence on commodity exports)\n",
        "   - Lack of economic diversification\n",
        "   - Dual economy problems (modern vs traditional sectors)\n",
        "\n",
        "3. **External Vulnerabilities**\n",
        "   - High exposure to economic shocks\n",
        "   - Limited access to international markets\n",
        "   - Currency instability\n",
        "\n",
        "4. **Social and Distributional Issues**\n",
        "   - High inequality limiting broad-based growth\n",
        "   - Large informal economy not captured in statistics\n",
        "   - Skills mismatch in labor markets\n",
        "\n",
        "#### **Model Performance Insights**\n",
        "\n",
        "1. **Error Patterns**: The distribution of errors reveals whether the model systematically over/underestimates certain types of economies\n",
        "\n",
        "2. **GDP Level Bias**: Analysis of absolute errors vs GDP level shows if the model struggles more with rich or poor countries\n",
        "\n",
        "3. **Outliers**: Countries with large prediction errors indicate unique circumstances not captured by standard macroeconomic indicators\n",
        "\n",
        "#### **Key Insight**\n",
        "\n",
        "Economic prosperity is **multidimensional** and cannot be fully explained by measurable indicators alone. The \"residual\" factors\u2014governance quality, institutional effectiveness, social capital, and policy execution\u2014play crucial roles. Countries with similar fundamentals can achieve vastly different outcomes based on **how** they use their resources, not just **what** resources they have.\n",
        "\n",
        "**This analysis using the full dataset provides a complete picture** of model performance across all countries, not just those held out for testing, giving us more reliable insights into systematic patterns of over- and under-performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. Conclusion & Key Findings\n",
        "\n",
        "### Summary\n",
        "\n",
        "This comprehensive analysis examined 40+ macroeconomic indicators across 20 countries (2010-2023) to understand and predict GDP per capita. Using the CRISP-DM methodology, we:\n",
        "\n",
        "1. **Identified key drivers of economic prosperity**: Technology infrastructure, education, and healthcare emerged as critical factors\n",
        "2. **Built predictive models**: Achieved strong performance (R\u00b2 > 0.85) using ensemble methods with carefully selected features\n",
        "3. **Uncovered insights**: Trade openness matters, but quality trumps quantity; some countries significantly outperform predictions\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "\u2705 **Technology infrastructure** (internet access, electricity) is the strongest predictor of GDP per capita\n",
        "\n",
        "\u2705 **Education investments** show significant returns, particularly at secondary and tertiary levels\n",
        "\n",
        "\u2705 **Trade openness** correlates with prosperity, but economic structure and export quality matter more than volume\n",
        "\n",
        "\u2705 **Model limitations** reveal that unmeasured factors (institutions, governance, policy quality) significantly impact outcomes\n",
        "\n",
        "\u2705 **Country-specific strategies** matter: identical indicators can yield different results based on execution and context\n",
        "\n",
        "### Recommendations for Policymakers\n",
        "\n",
        "1. **Prioritize digital infrastructure**: Invest heavily in internet access and reliable electricity\n",
        "2. **Focus on education quality**: Ensure high enrollment and completion rates at all levels\n",
        "3. **Develop export capabilities**: Build sophisticated export sectors, not just trade volume\n",
        "4. **Strengthen institutions**: Focus on governance quality and policy effectiveness\n",
        "5. **Learn from overperformers**: Study countries that exceed predictions to identify successful strategies\n",
        "\n",
        "### Future Work\n",
        "\n",
        "- Incorporate governance and institutional quality metrics\n",
        "- Analyze sector-specific contributions to GDP\n",
        "- Develop time-series forecasting models\n",
        "- Investigate causal relationships using advanced econometric methods\n",
        "- Expand dataset to include more countries and longer time periods\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ds-nd",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}